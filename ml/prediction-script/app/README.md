# ğŸŸ© ML Prediction Web App

*Containerization 101 Â· Machine Learning Docker Lab*

---

## ğŸ“Œ Overview

The **ML Prediction Web App** demonstrates how **machine learning logic can be packaged and deployed as a web application using Docker**.

Instead of focusing on complex ML algorithms, this project focuses on the **deployment mindset**:

* How ML logic is separated from UI
* How a backend connects ML logic to users
* How Docker makes ML applications **reproducible and portable**

> ğŸ¯ Goal: Help students understand how **ML code becomes a usable application** with Docker.

---

## ğŸ§  Learning Objectives

After completing this lab, you will be able to:

* Understand the difference between **ML logic and ML deployment**
* Connect ML code to a web interface
* Explain why ML projects often fail on different machines
* Use Docker to solve dependency and environment issues
* Build and run a Dockerized ML application

---

## ğŸ— Application Architecture

```text
User Input (Web UI)
        â†“
Flask Backend
        â†“
ML Logic (Prediction)
        â†“
Result shown on UI
```

All components run **inside a single Docker container**, ensuring consistency across systems.

---

## ğŸ“ Project Structure

```text
prediction-script/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py              # Backend server
â”‚   â”œâ”€â”€ model.py            # ML prediction logic
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html      # Frontend UI
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css           # UI styling
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker build instructions
â””â”€â”€ README.md
```

### Why this structure matters

* ML logic is **isolated and reusable**
* Backend controls input/output
* Frontend remains simple and clean
* Docker packages everything together

---

## â–¶ï¸ Run the App Without Docker (Optional)

Running locally helps you understand the flow **before containerization**.

```bash
python app/app.py
```

Open in browser:

```
http://localhost:5000
```

Enter a number and observe the prediction result.

---

## ğŸ³ Run the App With Docker

### Step 1ï¸âƒ£ Build the Docker Image

```bash
docker build -t awscc-ml-predict .
```

### Step 2ï¸âƒ£ Run the Docker Container

```bash
docker run -p 5000:5000 awscc-ml-predict
```

### Step 3ï¸âƒ£ Access the Application

Open:

```
http://localhost:5000
```

You are now using an **ML-powered web app running inside Docker**.

---

## ğŸ§ª Practice Tasks (Recommended)

To deepen your understanding, try the following:

* Modify the prediction logic in `model.py`
* Add new prediction categories
* Change UI colors and text
* Rebuild the Docker image after changes
* Observe how Docker ensures consistent behavior

---

## ğŸ¯ Key Takeaways

* ML code alone is **not enough**
* Deployment is as important as modeling
* Docker makes ML applications **portable**
* Containers solve dependency conflicts
* One image can run the same ML app everywhere

---

## ğŸ Final Note

This project represents a **real-world ML deployment pattern**:

* Simple model
* Backend API
* Frontend UI
* Dockerized environment

If you understand this flow, you are **thinking like an industry ML engineer**, not just a student.

---

### ğŸ³ Build Once. Run Anywhere.

---
